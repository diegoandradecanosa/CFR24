{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizadores en Pytorch\n",
    "\n",
    "En este  notebook vamos a prestar una especial atención a los optimizadores en Pytorch. \n",
    "\n",
    "Los optimizadores se encuentran agrupados dentro del paquete torch.optim. Su rol dentro del proceso de aprendizaje es integrarse con el mecanismo de autograd de los tensores y proporcionar una implementación de la pasada *backward*. Para ello, tienen que obtener información de cómo de lejos está el modelo de la salida deseada para realizar un cálculo de los gradientes a aplicar sobre los parámetros del modelo. Las funciones de pérdida se tratan de forma separada en otro notebook.\n",
    "\n",
    "Algunas características comunes a los optimizadores de Pytorch son:\n",
    "\n",
    "- Mantienen el estado actual y actualizan los parámetros usando un cálculo de sus gradientes\n",
    "- Tienen una interfaz común que facilita \n",
    "    - Que sean fácilmente intercambiables\n",
    "    - Que se pueden implementar optimizadores ad-hoc\n",
    "- Reciben un iterables con los parámetros aprendibles del modelo\n",
    "    - También reciben otros parámetros, llamados hiperparámetros del optimizador, que permite su configuración (ejs. learning rate, momentum)\n",
    "\n",
    "Métodos principales:\n",
    "\n",
    "- *backward()*: Calcula los gradientes. Se aplica a la función de pérdida, no al optimizador.\n",
    "- *zero_grad()*: Pone a 0 los gradientes del optimizador\n",
    "- *step()*: Aplica los gradientes calculados por *backward()* a los parámetros del modelo\n",
    "\n",
    "La clase base del optimizador es optim.Optimizer y recibe como parámetros:\n",
    "- Un interable que contiene los parámetros a optimizar\n",
    "- Un diccionario con los hiperparámetros del optimizador (lr, momentum, etc...)\n",
    "\n",
    "A continuación iremos viendo ejemplos ilustrativos del uso de los optimizadores más comunes: SGD, RMSProp, Adagrad, Adam, Adadelta.\n",
    "\n",
    "Empecemos por Stochastic Gradiente Descent (SGD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Época 1, Pérdida: 1.8640259816823408\n",
      "Época 2, Pérdida: 1.4053188300955937\n",
      "Época 3, Pérdida: 1.2420483254410726\n",
      "Época 4, Pérdida: 1.1423947820273201\n",
      "Época 5, Pérdida: 1.0663036326770587\n",
      "Época 6, Pérdida: 1.0077854537445565\n",
      "Época 7, Pérdida: 0.9586798157685857\n",
      "Época 8, Pérdida: 0.916822252066239\n",
      "Época 9, Pérdida: 0.8699657893775369\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 58\u001b[0m\n\u001b[1;32m     56\u001b[0m running_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# Para cada lote de datos\u001b[39;00m\n\u001b[0;32m---> 58\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, data \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(trainloader, \u001b[38;5;241m0\u001b[39m):\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;66;03m# Obtenemos las entradas y las etiquetas del lote del conjunto de entrenamiento\u001b[39;00m\n\u001b[1;32m     60\u001b[0m     inputs, labels \u001b[38;5;241m=\u001b[39m data\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;66;03m# Reiniciamos los gradientes\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/netapp2/Store_uni/home/ulc/es/dac/mypython/lib/python3.10/site-packages/torch/utils/data/dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/mnt/netapp2/Store_uni/home/ulc/es/dac/mypython/lib/python3.10/site-packages/torch/utils/data/dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m/mnt/netapp2/Store_uni/home/ulc/es/dac/mypython/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m/mnt/netapp2/Store_uni/home/ulc/es/dac/mypython/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m/mnt/netapp2/Store_uni/home/ulc/es/dac/mypython/lib/python3.10/site-packages/torchvision/datasets/cifar.py:118\u001b[0m, in \u001b[0;36mCIFAR10.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    115\u001b[0m img \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mfromarray(img)\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 118\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    121\u001b[0m     target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform(target)\n",
      "File \u001b[0;32m/mnt/netapp2/Store_uni/home/ulc/es/dac/mypython/lib/python3.10/site-packages/torchvision/transforms/transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[0;32m---> 95\u001b[0m         img \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "File \u001b[0;32m/mnt/netapp2/Store_uni/home/ulc/es/dac/mypython/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/netapp2/Store_uni/home/ulc/es/dac/mypython/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/netapp2/Store_uni/home/ulc/es/dac/mypython/lib/python3.10/site-packages/torchvision/transforms/transforms.py:277\u001b[0m, in \u001b[0;36mNormalize.forward\u001b[0;34m(self, tensor)\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, tensor: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m    270\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    271\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m    272\u001b[0m \u001b[38;5;124;03m        tensor (Tensor): Tensor image to be normalized.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[38;5;124;03m        Tensor: Normalized Tensor image.\u001b[39;00m\n\u001b[1;32m    276\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 277\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormalize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/netapp2/Store_uni/home/ulc/es/dac/mypython/lib/python3.10/site-packages/torchvision/transforms/functional.py:349\u001b[0m, in \u001b[0;36mnormalize\u001b[0;34m(tensor, mean, std, inplace)\u001b[0m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tensor, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[1;32m    347\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimg should be Tensor Image. Got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(tensor)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 349\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF_t\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormalize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmean\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstd\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/netapp2/Store_uni/home/ulc/es/dac/mypython/lib/python3.10/site-packages/torchvision/transforms/_functional_tensor.py:920\u001b[0m, in \u001b[0;36mnormalize\u001b[0;34m(tensor, mean, std, inplace)\u001b[0m\n\u001b[1;32m    918\u001b[0m mean \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mas_tensor(mean, dtype\u001b[38;5;241m=\u001b[39mdtype, device\u001b[38;5;241m=\u001b[39mtensor\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    919\u001b[0m std \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mas_tensor(std, dtype\u001b[38;5;241m=\u001b[39mdtype, device\u001b[38;5;241m=\u001b[39mtensor\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m--> 920\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43m(\u001b[49m\u001b[43mstd\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43many\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    921\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstd evaluated to zero after conversion to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdtype\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, leading to division by zero.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    922\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mean\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Importamos las bibliotecas necesarias\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Definimos las transformaciones para los conjuntos de datos\n",
    "transform = transforms.Compose([\n",
    "    # Transformamos las imágenes a tensores\n",
    "    transforms.ToTensor(),\n",
    "    # Normalizamos los tensores con media 0.5 y desviación estándar 0.5\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  \n",
    "])\n",
    "\n",
    "# Cargamos los conjuntos de datos CIFAR-10\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "trainloader = DataLoader(trainset, batch_size=64, shuffle=True)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "testloader = DataLoader(testset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Definimos una red convolucional simple\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)  # 3 canales de entrada para RGB, 6 de salida, kernel de 5x5\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)  # 10 clases de salida para CIFAR-10\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 16 * 5 * 5)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "model = Net()\n",
    "\n",
    "# Especificamos la función de pérdida y el optimizador\n",
    "# En CrossEntropyLoss, el mejor valor de la función de pérdida es 0\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# SGD: Stochastic Gradient Descent\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9, foreach=False, nesterov=False)\n",
    "\n",
    "# Finalmente, entrenamos el modelo\n",
    "# Durante 10 épocas\n",
    "for epoch in range(10):\n",
    "    running_loss = 0.0\n",
    "    # Para cada lote de datos\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # Obtenemos las entradas y las etiquetas del lote del conjunto de entrenamiento\n",
    "        inputs, labels = data\n",
    "        # Reiniciamos los gradientes\n",
    "        optimizer.zero_grad()\n",
    "        # Hacemos una pasada hacia adelante\n",
    "        outputs = model(inputs)\n",
    "        # Calculamos la pérdida\n",
    "        loss = criterion(outputs, labels)\n",
    "        # Hacemos una pasada hacia atrás\n",
    "        loss.backward()\n",
    "        # Actualizamos los parámetros\n",
    "        optimizer.step()\n",
    "        # Imprimimos estadísticas\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "    print(f'Época {epoch+1}, Pérdida: {running_loss/len(trainloader)}')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En la celda anterior, podemos hacer distintas ejecuciones cambiando los valores de los parámetros de la celda de construcción del optimizador:\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9, foreach=False, nesterov=False)\n",
    "\n",
    "A continuación, vamos a hacer lo mismo, pero cambiando el optimizador SGD, por Adagrad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# Importamos las bibliotecas necesarias\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "# Definimos las transformaciones para los conjuntos de datos\n",
    "transform = transforms.Compose([\n",
    "    # Transformamos las imágenes a tensores\n",
    "    transforms.ToTensor(),\n",
    "    # Normalizamos los tensores con media 0.5 y desviación estándar 0.5\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  \n",
    "])\n",
    "\n",
    "# Cargamos los conjuntos de datos CIFAR-10\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "trainloader = DataLoader(trainset, batch_size=64, shuffle=True)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "testloader = DataLoader(testset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Definimos una red convolucional simple\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)  # 3 canales de entrada para RGB, 6 de salida, kernel de 5x5\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)  # 10 clases de salida para CIFAR-10\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 16 * 5 * 5)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "model = Net()\n",
    "\n",
    "# Especificamos la función de pérdida y el optimizador\n",
    "# En CrossEntropyLoss, el mejor valor de la función de pérdida es 0\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Adagrad\n",
    "optimizer = optim.Adagrad(model.parameters(), lr=0.01, lr_decay=0, weight_decay=0, initial_accumulator_value=0, eps=1e-10)\n",
    "\n",
    "# Finalmente, entrenamos el modelo\n",
    "# Durante 10 épocas\n",
    "for epoch in range(10):\n",
    "    running_loss = 0.0\n",
    "    # Para cada lote de datos\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # Obtenemos las entradas y las etiquetas del lote del conjunto de entrenamiento\n",
    "        inputs, labels = data\n",
    "        # Reiniciamos los gradientes\n",
    "        optimizer.zero_grad()\n",
    "        # Hacemos una pasada hacia adelante\n",
    "        outputs = model(inputs)\n",
    "        # Calculamos la pérdida\n",
    "        loss = criterion(outputs, labels)\n",
    "        # Hacemos una pasada hacia atrás\n",
    "        loss.backward()\n",
    "        # Actualizamos los parámetros\n",
    "        optimizer.step()\n",
    "        # Imprimimos estadísticas\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "    print(f'Época {epoch+1}, Pérdida: {running_loss/len(trainloader)}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observamos cuál es la diferencia en tiempo de ejecución, y en el valor de la función de pérdida alcanzado después de las 10 epochs.\n",
    "\n",
    "Podemos hacer cambios en los parámetros del constructor del optimizador:\n",
    "\n",
    "optimizer = optim.Adagrad(model.parameters(), lr=0.01, lr_decay=0, weight_decay=0, initial_accumulator_value=0, eps=1e-10)\n",
    "\n",
    "y ver qué influencia tienen estos cambios en el tiempo de ejecución y en la precisión final alcanzada tras las 10 epochs. \n",
    "\n",
    "Ahora, vamos a probar sucesivamente los optimizadores RMSProp y Adam con el mismo ejemplo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RMSprop\n",
    "optimizer = optim.RMSprop(model.parameters(), lr=0.01, alpha=0.99, eps=1e-08, weight_decay=0, momentum=0, centered=False)\n",
    "# Finalmente, entrenamos el modelo\n",
    "# Durante 10 épocas\n",
    "for epoch in range(10):\n",
    "    running_loss = 0.0\n",
    "    # Para cada lote de datos\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # Obtenemos las entradas y las etiquetas del lote del conjunto de entrenamiento\n",
    "        inputs, labels = data\n",
    "        # Reiniciamos los gradientes\n",
    "        optimizer.zero_grad()\n",
    "        # Hacemos una pasada hacia adelante\n",
    "        outputs = model(inputs)\n",
    "        # Calculamos la pérdida\n",
    "        loss = criterion(outputs, labels)\n",
    "        # Hacemos una pasada hacia atrás\n",
    "        loss.backward()\n",
    "        # Actualizamos los parámetros\n",
    "        optimizer.step()\n",
    "        # Imprimimos estadísticas\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "    print(f'Época {epoch+1}, Pérdida: {running_loss/len(trainloader)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adam\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0, amsgrad=False)\n",
    "# Finalmente, entrenamos el modelo\n",
    "# Durante 10 épocas\n",
    "for epoch in range(10):\n",
    "    running_loss = 0.0\n",
    "    # Para cada lote de datos\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # Obtenemos las entradas y las etiquetas del lote del conjunto de entrenamiento\n",
    "        inputs, labels = data\n",
    "        # Reiniciamos los gradientes\n",
    "        optimizer.zero_grad()\n",
    "        # Hacemos una pasada hacia adelante\n",
    "        outputs = model(inputs)\n",
    "        # Calculamos la pérdida\n",
    "        loss = criterion(outputs, labels)\n",
    "        # Hacemos una pasada hacia atrás\n",
    "        loss.backward()\n",
    "        # Actualizamos los parámetros\n",
    "        optimizer.step()\n",
    "        # Imprimimos estadísticas\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "    print(f'Época {epoch+1}, Pérdida: {running_loss/len(trainloader)}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En los dos casos anteriores, podemos hacer las mismas pruebas de variación de hiperparámetros que hicimos con los otros dos optimizadores, y observar el efecto que tienen. \n",
    "\n",
    "## Ajuste del Learning Rate\n",
    "\n",
    "En los ejemplos anteriores, hemos fijado el *Learning Rate* (*lr*) en el momento de creación del optimizador, como un parámetro del mismo. Es cierto, que algunos de esos algoritmos van adaptando el *lr* de forma dinámica, en función de datos estadísticos del entrenamiento, pero también existe otro mecanismo que permite adaptar el learning rate dinámicamente, desde fuera del optimizador. Ambos mecanismos se pueden usar de forma conjunta, pero esto podría producir efectos inesperados. Lo habitual, sería utilizar estos mecanismo adaptación en conjunción con optimizadores como SGD que no incorporan un mecanismo de este tipo.\n",
    "\n",
    "Exploremos su uso, recuperando la versión SGD de nuestro ejemplo. En ella, usamos uno de estos planificadores (StepLR). A modo informativo, imprimimos el *lr* al final de cada epoch, para poder observar las actualizaciones realizadas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from torch.optim import lr_scheduler\n",
    "# SGD: Stochastic Gradient Descent\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9, foreach=False, nesterov=False)\n",
    "# Creamos un programador de tasa de aprendizaje\n",
    "# Cada 7 épocas, la tasa de aprendizaje se multiplicará por 0.1\n",
    "scheduler = lr_scheduler.StepLR(optimizer, step_size=2, gamma=0.1)\n",
    "\n",
    "# Finalmente, entrenamos el modelo\n",
    "# Durante 10 épocas\n",
    "for epoch in range(10):\n",
    "    running_loss = 0.0\n",
    "    # Para cada lote de datos\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # Obtenemos las entradas y las etiquetas del lote del conjunto de entrenamiento\n",
    "        inputs, labels = data\n",
    "        # Reiniciamos los gradientes\n",
    "        optimizer.zero_grad()\n",
    "        # Hacemos una pasada hacia adelante\n",
    "        outputs = model(inputs)\n",
    "        # Calculamos la pérdida\n",
    "        loss = criterion(outputs, labels)\n",
    "        # Hacemos una pasada hacia atrás\n",
    "        loss.backward()\n",
    "        # Actualizamos los parámetros usando el step del optimizador\n",
    "        optimizer.step()\n",
    "        # Imprimimos estadísticas\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "    print(f'Época {epoch+1}, Pérdida: {running_loss/len(trainloader)}')\n",
    "    # Actualizamos el lr usando el step del planifcador de lr\n",
    "    scheduler.step()\n",
    "    print(f'Época {epoch+1}, Pérdida: {running_loss/len(trainloader)}, Tasa de aprendizaje: {scheduler.get_lr()[0]}')\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probamos ahora, con un planificador más sofisticado: *CosineAnnealingLR*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from torch.optim import lr_scheduler\n",
    "# SGD: Stochastic Gradient Descent\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.9, foreach=False, nesterov=False)\n",
    "# Creamos un programador de tasa de aprendizaje\n",
    "# Cada 7 épocas, la tasa de aprendizaje se multiplicará por 0.1\n",
    "scheduler = lr_scheduler.CosineAnnealingLR(optimizer, T_max=2, eta_min=0)\n",
    "\n",
    "# Finalmente, entrenamos el modelo\n",
    "# Durante 10 épocas\n",
    "for epoch in range(10):\n",
    "    running_loss = 0.0\n",
    "    # Para cada lote de datos\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # Obtenemos las entradas y las etiquetas del lote del conjunto de entrenamiento\n",
    "        inputs, labels = data\n",
    "        # Reiniciamos los gradientes\n",
    "        optimizer.zero_grad()\n",
    "        # Hacemos una pasada hacia adelante\n",
    "        outputs = model(inputs)\n",
    "        # Calculamos la pérdida\n",
    "        loss = criterion(outputs, labels)\n",
    "        # Hacemos una pasada hacia atrás\n",
    "        loss.backward()\n",
    "        # Actualizamos los parámetros usando el step del optimizador\n",
    "        optimizer.step()\n",
    "        # Imprimimos estadísticas\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "    print(f'Época {epoch+1}, Pérdida: {running_loss/len(trainloader)}')\n",
    "    # Actualizamos el lr usando el step del planifcador de lr\n",
    "    scheduler.step()\n",
    "    print(f'Época {epoch+1}, Pérdida: {running_loss/len(trainloader)}, Tasa de aprendizaje: {scheduler.get_lr()[0]}')\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Promediado de pesos\n",
    "\n",
    "El promediado de pesos, Stochastic Weight Averaging (SWA) permite mejorar la capacidad de generalización del modelo, promediando sus pesos durante un determinado número de iteraciones. \n",
    "\n",
    "El promedio se hace en base a varios puntos de la trayectoria de optimización. Vamos a ver un ejemplo comentado del uso de esta técnica en el ejemplo que hemos uso durante este notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from torch.optim import lr_scheduler\n",
    "# SGD: Stochastic Gradient Descent\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9, foreach=False, nesterov=False)\n",
    "# Creamos un programador de tasa de aprendizaje\n",
    "# Cada 7 épocas, la tasa de aprendizaje se multiplicará por 0.1\n",
    "swa_model=torch.optim.swa_utils.AveragedModel(model)\n",
    "scheduler = lr_scheduler.StepLR(optimizer, step_size=2, gamma=0.1)\n",
    "swa_start=5\n",
    "swa_scheduler = torch.optim.swa_utils.SWALR(optimizer, swa_lr=0.05)\n",
    "\n",
    "# Finalmente, entrenamos el modelo\n",
    "# Durante 10 épocas\n",
    "for epoch in range(10):\n",
    "    running_loss = 0.0\n",
    "    # Para cada lote de datos\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # Obtenemos las entradas y las etiquetas del lote del conjunto de entrenamiento\n",
    "        inputs, labels = data\n",
    "        # Reiniciamos los gradientes\n",
    "        optimizer.zero_grad()\n",
    "        # Hacemos una pasada hacia adelante\n",
    "        outputs = model(inputs)\n",
    "        # Calculamos la pérdida\n",
    "        loss = criterion(outputs, labels)\n",
    "        # Hacemos una pasada hacia atrás\n",
    "        loss.backward()\n",
    "        # Actualizamos los parámetros usando el step del optimizador\n",
    "        optimizer.step()\n",
    "        # Imprimimos estadísticas\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "    print(f'Época {epoch+1}, Pérdida: {running_loss/len(trainloader)}')\n",
    "    # Actualizamos el lr usando el step del planifcador de lr\n",
    "    if epoch>swa_start:\n",
    "        swa_model.update_parameters(model)\n",
    "        swa_scheduler.step()\n",
    "        print(f'SWA: Época {epoch+1}, Pérdida: {running_loss/len(trainloader)}, Tasa de aprendizaje: {swa_scheduler.get_lr()[0]}')\n",
    "\n",
    "    else:\n",
    "        scheduler.step()\n",
    "        print(f'Época {epoch+1}, Pérdida: {running_loss/len(trainloader)}, Tasa de aprendizaje: {scheduler.get_lr()[0]}')\n",
    "\n",
    "torch.optim.swa_utils.update_bn(trainloader, swa_model)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejercicio\n",
    "\n",
    "Ahora vamos a hacer un ejercicio sobre optimizadores. El objetivo es probar distintos algoritmos de optimización sobre un mismo ejemplo y comparar su desempeño.\n",
    "El código está preparado para entrenar la misma red con tres algoritmos de optimización distintos: SGD, Adam y RMSProp. La parte del código que falta es la definición del uso de los optimizadores Adam y RMSProp, proporcionándose el código de SGD como referencia.\n",
    "\n",
    "Al final de los tres entrenamientos, se imprime una gráfica comparando la evolución de la función de pérdida en el entrenamiento, cuando se usan cada uno de los 3 optimizadores.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load CIFAR-10 dataset\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=4,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "# Define the network architecture\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 16 * 5 * 5)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Define the loss function\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "\n",
    "# Define the training function\n",
    "def train(net, trainloader, optimizer, num_epochs=2):\n",
    "    loss_values = []\n",
    "    for epoch in range(num_epochs):  # loop over the dataset multiple times\n",
    "        running_loss = 0.0\n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            inputs, labels = data\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = net(inputs)\n",
    "            loss = loss_func(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # save losses to plot later\n",
    "            running_loss += loss.item()\n",
    "            if i % 2000 == 1999:  # print every 2000 mini-batches\n",
    "                loss_values.append(running_loss / 2000)\n",
    "                running_loss = 0.0\n",
    "    return loss_values\n",
    "\n",
    "# SGD\n",
    "net_SGD = Net()\n",
    "optimizer_SGD = optim.SGD(net_SGD.parameters(), lr=0.001, momentum=0.9)\n",
    "loss_SGD = train(net_SGD, trainloader, optimizer_SGD)\n",
    "\n",
    "# Adam\n",
    "net_Adam = Net()\n",
    "# Escribe tu código aquí\n",
    "# ...\n",
    "\n",
    "\n",
    "# RMSprop\n",
    "net_RMSprop = Net()\n",
    "# Escribe tu código aquí\n",
    "# ...\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.plot(loss_SGD, label='SGD')\n",
    "plt.plot(loss_Adam, label='Adam')\n",
    "plt.plot(loss_RMSprop, label='RMSprop')\n",
    "plt.xlabel('Epochs (x2000 mini-batches)')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss Evolution with Different Optimizers')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mypython",
   "language": "python",
   "name": "mypython"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
